<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lorenz Hufe - PhD Student</title>
    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>

<body>


    <div class="container">
        <!-- Header -->
        <header class="header">
            <div class="profile-section">
                <div class="profile-content">
                    <div class="profile-image">
                        <img src="image.png" alt="Lorenz Hufe" id="profile-img">
                    </div>
                    <div class="profile-info">
                        <h1 class="name">Lorenz Hufe</h1>
                        <p class="title">ELLIS PhD Student at <br> Fraunhofer HHI and <br> University of Oxford</p>
                        <div class="contact-links">
                            <a href="mailto:lorenz.hufe@bliss.berlin" class="contact-link">
                                <i class="fas fa-envelope"></i>
                            </a>
                            <a href="https://linkedin.com/in/lorenz-hufe" class="contact-link">
                                <i class="fab fa-linkedin"></i>
                            </a>
                            <a href="https://github.com/lowlorenz" class="contact-link">
                                <i class="fab fa-github"></i>
                            </a>
                            <a href="https://scholar.google.com/citations?user=BlI-rmsAAAAJ&hl=de&oi=ao"
                                class="contact-link">
                                <i class="fas fa-graduation-cap"></i>
                            </a>
                        </div>
                    </div>
                </div>

                <!-- Lorenz Attractor -->
                <div class="lorenz-attractor">
                    <div class="lorenz-canvas" id="lorenz-canvas"></div>
                </div>
            </div>
        </header>

        <!-- Navigation Tabs -->
        <nav class="tabs">
            <button class="tab-button active" onclick="openTab(event, 'home')">Home</button>
            <button class="tab-button" onclick="openTab(event, 'blog')">Blog</button>
        </nav>

        <!-- Main Content -->
        <main class="main-content">
            <!-- Home Tab -->
            <div id="home" class="tab-content active">
                <!-- About Section -->
                <section class="section">
                    <h2>About</h2>
                    <p>
                        I am an ELLIS PhD student at Fraunhofer Heinrich Hertz Institute (HHI), where I work
                        under the supervision of Prof. Wojciech Samek and the University of Oxford where I am supervised
                        by Dr. Christian Schr√∂der de Witt.
                        My research focuses on mechanistic interpretability, multimodal foundation models and
                        typographic understanding.
                    </p>
                    <p>
                        I co-founded <a href="https://bliss.berlin" target="_blank">Bliss e.V.</a>, a non-profit focused
                        on AI research. Furthermore I am a core contributor to the <a
                            href="https://github.com/Prisma-Multimodal/ViT-Prisma" target="_blank">ViT-Prisma</a>
                        libary for mechanistic interpretability in vision and video.
                    </p>
                    <p>
                        I have a hard time understanding why people decide to be homophobic, why <a
                            href="https://github.com/baskerville/bspwm" target="_blank">bspwm</a> is not the standard
                        windows manager, why we are not freaking out more about the climate
                        crisis or the demographic crisis and let oil concerns do whatever they want to.
                    </p>
                    <p>
                        If you want to get in touch with me, please do. I am happy to help if I can.
                    </p>
                </section>

                <!-- Publications -->
                <section class="section">
                    <h2>Publications</h2>
                    <div class="publications">
                        <div class="publication">
                            <h3><a href="https://arxiv.org/abs/2505.20229" class="paper-title" target="_blank">From What
                                    to How: Attributing CLIP's Latent Components Reveals Unexpected Semantic
                                    Reliance</a></h3>
                            <p class="authors"><span class="author-name">M Dreyer</span>, <span
                                    class="author-name"><strong>L Hufe</strong></span>, <span class="author-name">J
                                    Berend</span>, <span class="author-name">T Wiegand</span>, <span
                                    class="author-name">S Lapuschkin</span>, <span class="author-name">W
                                    Samek</span><br><span class="venue"> arXiv preprint arXiv:2505.20229, 2025</span>
                            </p>
                            <p class="description">
                                This work investigates the internal mechanisms of CLIP vision transformers, revealing
                                how different components contribute to semantic understanding. We use mechanistic
                                interpretability techniques to
                                uncover unexpected reliance patterns in multimodal representations.
                            </p>
                        </div>

                        <div class="publication">
                            <h3><a href="https://arxiv.org/abs/2504.19475" class="paper-title" target="_blank">Prisma:
                                    An Open Source Toolkit for Mechanistic Interpretability in Vision and Video</a></h3>
                            <p class="authors"><span class="author-name">Sonia Joseph</span>, <span
                                    class="author-name">Praneet Suresh</span>, <span class="author-name"><strong>Lorenz
                                        Hufe</strong></span>, <span class="author-name">Edward Stevinson</span>, <span
                                    class="author-name">Robert Graham</span>, <span class="author-name">Yash
                                    Vadi</span>, <span class="author-name">Danilo Bzdok</span>, <span
                                    class="author-name">Sebastian Lapuschkin</span>, <span class="author-name">Lee
                                    Sharkey</span>, <span class="author-name">Blake Aaron Richards</span><br><span
                                    class="venue">Oral presentation at Mechanistic Interpretability for Vision Workshop
                                    @ CVPR 2025</span></p>
                            <p class="description">
                                An open-source toolkit designed to facilitate mechanistic interpretability research in
                                computer vision and video understanding. Prisma provides researchers with tools to
                                analyze and understand the internal workings
                                of vision models.
                            </p>
                        </div>

                        <div class="publication">
                            <h3><a href="https://arxiv.org/abs/2504.08729" class="paper-title" target="_blank">Steering
                                    CLIP's Vision Transformer with Sparse Autoencoders</a></h3>
                            <p class="authors"><span class="author-name">Sonia Joseph</span>, <span
                                    class="author-name">Praneet Suresh</span>, <span class="author-name">Ethan
                                    Goldfarb</span>, <span class="author-name"><strong>Lorenz Hufe</strong></span>,
                                <span class="author-name">Yossi Gandelsman</span>, <span class="author-name">Robert
                                    Graham</span>, <span class="author-name">Danilo Bzdok</span>, <span
                                    class="author-name">Wojciech Samek</span>, <span class="author-name">Blake Aaron
                                    Richards</span><br><span class="venue">Mechanistic Interpretability for Vision
                                    Workshop @ CVPR 2025</span>
                            </p>
                            <p class="description">
                                We present a novel approach to controlling CLIP's vision transformer using sparse
                                autoencoders.
                                This work demonstrates how to steer model representations while maintaining performance,
                                contributing to mechanistic interpretability research.
                            </p>
                        </div>

                        <div class="publication">
                            <h3><a href="https://arxiv.org/abs/2504.04893" class="paper-title" target="_blank">SCAM: A
                                    Real-World Typographic Robustness Evaluation for Multimodal Foundation Models</a>
                            </h3>
                            <p class="authors"><span class="author-name">J Westerhoff</span>, <span
                                    class="author-name">E Purelku</span>, <span class="author-name">J Hackstein</span>,
                                <span class="author-name">J Loos</span>, <span class="author-name">L Pinetzki</span>,
                                <span class="author-name"><strong>L Hufe</strong></span><br><span class="venue">2nd
                                    Workshop on Emergent Visual Abilities and Limits of Foundation Models @ CVPR
                                    2025</span>
                            </p>
                            <p class="description">
                                A comprehensive evaluation framework for testing the robustness of multimodal foundation
                                models against typographic variations. This work addresses real-world challenges in
                                deploying robust AI
                                systems.
                            </p>
                        </div>

                        <div class="publication">
                            <h3><a href="https://aclanthology.org/2022.wmt-1.95/" class="paper-title"
                                    target="_blank">Experimental Machine Translation of the Swiss German Sign Language
                                    via 3D Augmentation of Body Keypoints</a></h3>
                            <p class="authors"><span class="author-name"><strong>L Hufe</strong></span>, <span
                                    class="author-name">E Avramidis</span><br><span class="venue">Proceedings of the
                                    Seventh Conference on Machine Translation (WMT), 983-988, 2022</span></p>
                            <p class="description">
                                This work presents a novel approach to Swiss German sign language translation using 3D
                                augmentation techniques on body keypoints. The research contributes to accessibility
                                technology and cross-modal
                                translation.
                            </p>
                        </div>
                    </div>
                </section>

                <!-- Blog References -->
                <section class="section">
                    <h2>Recent Blog Posts</h2>
                    <div class="blog-references">
                        <div class="blog-ref">
                            <h3><a href="#" onclick="openTab(event, 'blog'); showBlog('blog1')">Understanding Deep
                                    Learning Fundamentals</a></h3>
                            <p class="blog-date">December 2024</p>
                            <p>An introduction to the core concepts of deep learning and their practical applications...
                            </p>
                        </div>
                        <div class="blog-ref">
                            <h3><a href="#" onclick="openTab(event, 'blog'); showBlog('blog2')">My Research Journey at
                                    Fraunhofer HHI</a></h3>
                            <p class="blog-date">November 2024</p>
                            <p>Reflections on my experience as a PhD student and the exciting projects I'm working on...
                            </p>
                        </div>
                    </div>
                </section>
            </div>

            <!-- Blog Tab -->
            <div id="blog" class="tab-content">
                <div class="blog-navigation">
                    <button class="blog-nav-btn active" onclick="showBlog('blog1')">Blog Post 1</button>
                    <button class="blog-nav-btn" onclick="showBlog('blog2')">Blog Post 2</button>
                </div>

                <!-- Blog Post 1 -->
                <article id="blog1" class="blog-post active">
                    <h2>Understanding Deep Learning Fundamentals</h2>
                    <p class="blog-meta">Published on December 15, 2024</p>

                    <p>
                        Deep learning has revolutionized the field of artificial intelligence, enabling breakthrough
                        advances in computer vision, natural language processing, and many other domains. In this post,
                        I'll walk you through the fundamental concepts that make deep learning so powerful.
                    </p>

                    <h3>What is Deep Learning?</h3>
                    <p>
                        Deep learning is a subset of machine learning that uses neural networks with multiple layers
                        (hence "deep") to model and understand complex patterns in data. Unlike traditional machine
                        learning approaches that require manual feature engineering, deep learning models can
                        automatically learn hierarchical representations of data.
                    </p>

                    <h3>Key Components</h3>
                    <ul>
                        <li><strong>Neural Networks:</strong> The building blocks of deep learning systems</li>
                        <li><strong>Activation Functions:</strong> Functions that introduce non-linearity</li>
                        <li><strong>Backpropagation:</strong> The algorithm for training neural networks</li>
                        <li><strong>Optimization:</strong> Methods for finding the best model parameters</li>
                    </ul>

                    <h3>Applications in My Research</h3>
                    <p>
                        In my work at Fraunhofer HHI, I apply these deep learning principles to computer vision and
                        mechanistic interpretability. The ability of deep networks to learn complex representations has
                        been crucial for
                        understanding how models like CLIP process and understand visual information.
                    </p>

                    <p>
                        Understanding these fundamentals is essential for anyone working in modern AI research.
                        They provide the foundation for more advanced topics like transformers, generative models,
                        and specialized architectures for different domains.
                    </p>
                </article>

                <!-- Blog Post 2 -->
                <article id="blog2" class="blog-post">
                    <h2>My Research Journey at Fraunhofer HHI</h2>
                    <p class="blog-meta">Published on November 20, 2024</p>

                    <p>
                        As a PhD student at Fraunhofer HHI, I've had the opportunity to work on cutting-edge research
                        in computer vision and mechanistic interpretability. This post reflects on my journey and the
                        exciting projects I'm involved with.
                    </p>

                    <h3>Current Research Focus</h3>
                    <p>
                        My research primarily focuses on understanding and improving how AI models process visual
                        information. I'm particularly interested in mechanistic interpretability - understanding the
                        internal workings of models like CLIP and how they form their representations.
                    </p>

                    <h3>Recent Projects</h3>
                    <ul>
                        <li>Developing tools for analyzing vision transformer mechanisms</li>
                        <li>Investigating semantic relationships in multimodal models</li>
                        <li>Working on robustness evaluation for foundation models</li>
                    </ul>

                    <p>
                        The collaborative environment at Fraunhofer HHI has been instrumental in advancing these
                        projects. Working with talented researchers and having access to state-of-the-art resources
                        has helped push our research forward.
                    </p>
                </article>
            </div>
        </main>
    </div>
    <script src="script.js"></script>
</body>

</html>